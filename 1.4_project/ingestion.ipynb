{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mysql-connector\n",
    "# pip install --upgrade mysql-connector-python\n",
    "# pip install PyMySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/81hplzs570d_45ldwbxvmx4w0000gp/T/ipykernel_41370/192552451.py:21: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional, Union, Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import (\n",
    "    Boolean, BigInteger, Column, create_engine, DateTime, Float, \n",
    "    ForeignKey, Integer, PrimaryKeyConstraint, String, text\n",
    ")\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.schema import CreateTable\n",
    "from sqlalchemy import MetaData, Table # Delete later\n",
    "\n",
    "from database_loader import DatabaseLoader\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleaning\n",
    "# Index sorting by deploying sort_index() method. It will provide an easy access to data\n",
    "# Renaming of multiple columns using rename() method\n",
    "# Evaluating the presence duplicate values in dataframes\n",
    "# Creating and assigning values to columns\n",
    "# Converting row values of column into titlecase by using .str.title() and .apply() methods\n",
    "# Checking for weird symbols\n",
    "# Mapping dtypes to correct dtypes, i.e. Seasons to INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decorator to track MySQL DB tables creation\"\"\"\n",
    "def track_tables(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        table_name = getattr(result, 'table_name', func.__tablename__)\n",
    "        \n",
    "        # Check if table_name is already in db_tables and add if not. \n",
    "        if table_name not in db_tables:\n",
    "            db_tables.append(table_name)\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "  \n",
    "db_tables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes:\n",
    "class DataCleaning: # DataframeCleaner\n",
    "    \"\"\"\n",
    "    A class designed to perform various data cleaning operations on a pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Constructor for the DataCleaning class.\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.series = None\n",
    "        self.default_value = 0\n",
    "\n",
    "    def replace_to_none(self,  value: Union[str, List[str], None] = None):\n",
    "        \"\"\"\n",
    "        Replace specified values (or NaN by default) in the DataFrame with None.\n",
    "        \"\"\"\n",
    "        if value is None:\n",
    "            value = np.nan        \n",
    "\n",
    "        if not isinstance(value, list):  # If a single string is provided, convert it to a list.\n",
    "            value = [value]\n",
    "\n",
    "        for replace in value:\n",
    "            self.data.replace({replace: None}, inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Remove duplicate rows from the DataFrame.\n",
    "        \"\"\"\n",
    "        self.data.drop_duplicates(keep='first', inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    def remove_column_duplicates(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Remove duplicate value if any found. Keep rows with the lowest price. \n",
    "        \"\"\"\n",
    "        if column_name not in self.data.columns:\n",
    "            raise ValueError(f\"The DataFrame does not have a '{column_name}' column.\")\n",
    "\n",
    "        # Define columns to be considered for identifying duplicates.\n",
    "        column_subset = self.data.columns.difference([column_name])\n",
    "\n",
    "        # Sort DataFrame based on the 'price' column in ascending order.\n",
    "        self.data.sort_values(by=column_name, inplace=True)\n",
    "\n",
    "        # Drop duplicates, retaining the first occurrence (which will have the lowest price due to sorting).\n",
    "        self.data.drop_duplicates(subset=column_subset, keep='first', inplace=True)\n",
    "        return self.data\n",
    "\n",
    "    # def remove_brackets(self, column_name: str):\n",
    "    #     \"\"\"\n",
    "    #     Removes [ and ] from a specified column in a DataFrame.\n",
    "    #     \"\"\"\n",
    "    #     modified_column = self.data[column_name].str.replace('[\\[\\]]', '', regex=True)\n",
    "    #     return modified_column\n",
    "\n",
    "    # def remove_single_quotes(self, column_name: List[str]):\n",
    "    #     \"\"\"\n",
    "    #     Removes single quotes ' from a specified column in a DataFrame.\n",
    "    #     \"\"\"\n",
    "    #     modified_column = self.data[column_name].str.replace('\\'', '')\n",
    "    #     return modified_column\n",
    "\n",
    "    # Number cleaning\n",
    "    def number_cleaning(self, column_name: str, dtype='float'):\n",
    "        \"\"\"\n",
    "        Utility to clean and convert columns in a DataFrame into numeric types.\n",
    "        \n",
    "        Methods:\n",
    "        - number_cleaning: Converts a column into either float or int type.\n",
    "        - _extract_float: Helper to extract/convert values to float.\n",
    "        - _extract_int: Helper to extract/convert values to int.\n",
    "        \"\"\"\n",
    "        if column_name not in self.data:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "        self.series = self.data[column_name]\n",
    "\n",
    "        if dtype == 'float':\n",
    "            self.data[column_name] = self.series.apply(self._extract_float)\n",
    "        elif dtype == 'int':\n",
    "            self.data[column_name] = self.series.apply(self._extract_int)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dtype {dtype}\")\n",
    "        return self.data[column_name]\n",
    "\n",
    "    def _extract_float(self, value):\n",
    "        \"\"\"Attempt to extract or convert a value into a float.\"\"\"\n",
    "        if pd.isna(value):  # Check if value is NaN\n",
    "            return self.default_value\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            # If direct conversion fails, first remove all but the last period\n",
    "            modified_value = value[::-1].replace('.', '', value.count('.') - 1)[::-1]\n",
    "            # If direct conversion fails, extract the number from the string\n",
    "            float_val = ''.join(filter(lambda x: x.isdigit() or x == '.', modified_value))\n",
    "            if float_val:\n",
    "                return float(float_val)\n",
    "            else:\n",
    "                index_value = self.series[self.series == value].index[0]\n",
    "                print(f\"Failed to convert value '{value}' at index {index_value} to float.\")\n",
    "                return self.default_value\n",
    "\n",
    "    def _extract_int(self, value):\n",
    "        \"\"\"Attempt to extract or convert a value into an integer.\"\"\"\n",
    "        if pd.isna(value):  # Check if value is NaN\n",
    "            return self.default_value\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            # If direct conversion fails, extract the number from the string\n",
    "            int_val = ''.join(filter(lambda x: x.isdigit(), str(value)))\n",
    "            if int_val:\n",
    "                return int(int_val)\n",
    "            else:\n",
    "                index_value = self.series[self.series == value].index[0]\n",
    "                print(f\"Failed to convert value '{value}' at index {index_value} to int.\")\n",
    "                return self.default_value\n",
    "            \n",
    "# NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW  NEW \n",
    "    def remove_brackets_and_quotes(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Removes [ ], and ' from a specified column in a DataFrame.\n",
    "        \"\"\"\n",
    "        self.data[column_name] = self.data[column_name].str.replace('[\\[\\]\\'\\']', '', regex=True)\n",
    "        return self.data[column_name]  # Return only the processed column\n",
    "    \n",
    "    def missing_to_zero(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Replace missing values in the specified column with 0 if it's a numeric column,\n",
    "        or \"None\" if it's a String/Object column.\n",
    "        \"\"\"\n",
    "        if column_name not in self.data.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(self.data[column_name]):\n",
    "            self.data[column_name].fillna(0, inplace=True)\n",
    "        else:\n",
    "            self.data[column_name].fillna(\"None\", inplace=True)\n",
    "            self.data[column_name].replace(\"\", \"None\", inplace=True)\n",
    "        return self.data[column_name]  # Return only the processed column\n",
    "\n",
    "    def missing_to_median_or_mode(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Replace missing values in the specified column with median if it's a numeric column,\n",
    "        or mode if it's a String/Object column.\n",
    "        \"\"\"\n",
    "        if column_name not in self.data.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(self.data[column_name]):\n",
    "            self.data[column_name].fillna(self.data[column_name].median(), inplace=True)\n",
    "        else:\n",
    "            mode_value = self.data[column_name].mode().iloc[0]\n",
    "            self.data[column_name].fillna(mode_value, inplace=True)\n",
    "        return self.data[column_name]  # Return only the processed column\n",
    "    \n",
    "    def generate_factor_id(self, id_columns: list, id_field: str):\n",
    "        \"\"\"\n",
    "        Generate primary keys from specified id_columns in the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            id_columns (list): List of column names from which primary keys are generated.\n",
    "            id_field (str): The name of the new primary key column.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with primary key columns added.\n",
    "\n",
    "        Description:\n",
    "            The method generates primary keys for the DataFrame based on the specified 'id_columns'.\n",
    "            It uses the 'pd.factorize()' function to convert categorical data into numerical codes.\n",
    "            The method creates unique integer identifiers for each combination of values in the selected 'id_columns'.\n",
    "            The primary key values start from 1 and increment by 1 for each unique combination of values.\n",
    "            The new primary key column is added to the DataFrame with the name specified by 'id_field'.\n",
    "        \"\"\"\n",
    "        # Sort the DataFrame by its index\n",
    "        self.data.sort_index(inplace=True)\n",
    "        \n",
    "        # Generate the ID key for the DataFrame\n",
    "        self.data[id_field] = pd.factorize(self.data[id_columns].apply(tuple, axis=1))[0] + 1\n",
    "        return self.data\n",
    "    \n",
    "    def add_id(df):\n",
    "        \"\"\"\n",
    "        Add an 'id' column to the DataFrame based on the index.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to which the 'id' column will be added.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the 'id' column added based on the index.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input is not a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input should be a pandas DataFrame.\")\n",
    "\n",
    "        # Reset the DataFrame index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Add the 'id' column based on the index, starting from 1\n",
    "        df['id'] = df.index + 1\n",
    "\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def compute_missing_values(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the number and percentage of missing values for each column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to analyze for missing values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing counts and percentages of missing values for each column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for missing values in the dataframe\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_values_percentage = round((data.isnull().sum() / len(data)) * 100, 2)\n",
    "\n",
    "    # Combine the counts and percentages into a dataframe for a clearer view\n",
    "    missing_values_df = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Percentage (%)': missing_values_percentage\n",
    "    }).sort_values(by='Percentage (%)', ascending=False)\n",
    "    \n",
    "    # Filter to include only columns where missing value count > 0\n",
    "    # missing_values_df = missing_values_df[missing_values_df['Missing Values'] > 0].sort_values(by='Percentage (%)', ascending=False)\n",
    "    missing_values_df = missing_values_df.sort_values(by='Percentage (%)', ascending=False)\n",
    "    \n",
    "    return missing_values_df\n",
    "\n",
    "def not_alphanumeric_columns(data: pd.DataFrame, pattern=r\"[^a-zA-Z0-9\\s]\") -> list:\n",
    "    \"\"\"\n",
    "    Identify columns with non-alphanumeric characters in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to analyze.\n",
    "        pattern (str, optional): Regular expression pattern to search for. Defaults to non-alphanumeric characters.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of column names that contain entries with non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_with_symbols = []\n",
    "    \n",
    "    for column in data.columns:\n",
    "        if data[data[column].astype(str).str.contains(pattern, na=False, regex=True)].shape[0] > 0:\n",
    "            columns_with_symbols.append(column)\n",
    "    \n",
    "    return columns_with_symbols\n",
    "\n",
    "def not_alphanumeric(data: pd.DataFrame, column: str, pattern=r\"[^a-zA-Z0-9\\s]\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    View entries with non-alphanumeric characters for a specific column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to analyze.\n",
    "        column (str): The column in which to search for non-alphanumeric characters.\n",
    "        pattern (str, optional): Regular expression pattern to search for. Defaults to non-alphanumeric characters.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series containing entries from the specified column that match the pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    return data[data[column].astype(str).str.contains(pattern, na=False, regex=True)][column]\n",
    "\n",
    "def outliers_numerical_cols(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Visualize outliers in numerical columns using boxplots.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "    \"\"\"\n",
    "    numerical_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    num_cols = 3\n",
    "    num_rows = (len(numerical_columns) + 2) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "    for i, col in enumerate(numerical_columns, start=1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        sns.boxplot(x=df[col].dropna())\n",
    "        plt.title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def distribution_numerical_cols(df: pd.DataFrame, kde: bool = True) -> None:\n",
    "    \"\"\"Visualize distribution of numerical columns using histograms.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        kde (bool, optional): Whether to plot a KDE (Kernel Density Estimation). Defaults to True.\n",
    "    \"\"\"\n",
    "    numerical_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    num_cols = 3\n",
    "    num_rows = (len(numerical_columns) + 2) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "    for i, col in enumerate(numerical_columns, start=1):\n",
    "        try:\n",
    "            plt.subplot(num_rows, num_cols, i)\n",
    "            sns.histplot(df[col], bins=30, kde=kde)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def outlier_cols(column_data: pd.Series) -> bool:\n",
    "    \"\"\"Check if a column has outliers using IQR.\n",
    "\n",
    "    Args:\n",
    "        column_data (pd.Series): Column data to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if outliers exist, False otherwise.\n",
    "    \"\"\"\n",
    "    Q1 = column_data.quantile(0.25)\n",
    "    Q3 = column_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return ((column_data < lower_bound) | (column_data > upper_bound)).any()\n",
    "\n",
    "def analyze_categorical_column(\n",
    "    dataframe: pd.DataFrame, column_name: str\n",
    ") -> Tuple[Dict[str, Any], pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Analyze a categorical column of a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input dataframe containing the column to be analyzed.\n",
    "        column_name (str): Name of the categorical column to analyze.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Any], pd.DataFrame, pd.DataFrame, List[str]]: A tuple containing\n",
    "        analysis results as a dictionary,\n",
    "        dataframes with the shortest and longest values,\n",
    "        and a list of non-string columns.\n",
    "    \"\"\"\n",
    "    analysis_results = {}\n",
    "    non_str_columns = []\n",
    "\n",
    "    if column_name not in dataframe.columns:\n",
    "        return f\"The column '{column_name}' does not exist in the DataFrame.\"\n",
    "\n",
    "    analysis_results[\"missing_values\"] = dataframe[column_name].isnull().sum()\n",
    "    analysis_results[\"unique_values\"] = dataframe[column_name].nunique()\n",
    "    analysis_results[\"total_values\"] = dataframe[column_name].count()\n",
    "    analysis_results[\"duplicated_values\"] = dataframe[column_name].duplicated().sum()\n",
    "    analysis_results[\"values_with_whitespace\"] = (\n",
    "        dataframe[column_name].str.strip().ne(dataframe[column_name]).sum()\n",
    "    )\n",
    "    analysis_results[\"values_with_unusual_chars\"] = (\n",
    "        dataframe[column_name]\n",
    "        .apply(lambda x: any(ord(char) < 32 or ord(char) > 126 for char in str(x)))\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    value_lengths = dataframe[column_name].str.len()\n",
    "    analysis_results[\"shortest_value_length\"] = value_lengths.min()\n",
    "    analysis_results[\"longest_value_length\"] = value_lengths.max()\n",
    "    analysis_results[\"average_value_length\"] = value_lengths.mean()\n",
    "\n",
    "    shortest_value = dataframe[\n",
    "        dataframe[column_name].str.len() == analysis_results[\"shortest_value_length\"]\n",
    "    ]\n",
    "    longest_value = dataframe[\n",
    "        dataframe[column_name].str.len() == analysis_results[\"longest_value_length\"]\n",
    "    ]\n",
    "\n",
    "    return analysis_results, shortest_value, longest_value\n",
    "\n",
    "def column_max_lengths(dataframe: pd.DataFrame, categorical_columns: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Display the maximum string lengths of specified columns in a given dataframe.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The dataframe containing the columns to be analyzed.\n",
    "        columns (List[str]): List of column names whose maximum string lengths are to be displayed.\n",
    "        \n",
    "    Returns:\n",
    "        None: The function prints the results and does not return any value.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Column Name\".ljust(30), \"Max Length\".ljust(10))\n",
    "    print(\"-\" * 40)\n",
    "    for column in categorical_columns:\n",
    "        max_length = dataframe[column].str.len().max()\n",
    "        print(f\"{column.ljust(30)} {str(max_length).ljust(10)}\")\n",
    "        \n",
    "def df_col_max_lengths(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Display the maximum string lengths of categorical columns in a given dataframe.\n",
    "    \"\"\"\n",
    "    categorical_columns = dataframe.select_dtypes(include=['string', 'object']).columns.tolist()\n",
    "    \n",
    "    print(\"Column Name\".ljust(30), \"Max Length\".ljust(10))\n",
    "    print(\"-\" * 40)\n",
    "    for column in categorical_columns:\n",
    "        max_length = dataframe[column].str.len().max()\n",
    "        print(f\"{column.ljust(30)} {str(max_length).ljust(10)}\")\n",
    "                \n",
    "\n",
    "\n",
    "def lowercase_column_names(df):\n",
    "    \"\"\"Convert all column names in a DataFrame to lowercase.\"\"\"\n",
    "    # Use a list comprehension to convert column names to lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "  # Usage: df = lowercase_column_names(df)\n",
    "  # Works same as: df.columns = [c.lower() for c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to track DataFrame creation\n",
    "def track_dataframe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Get the result of the function (which should be a DataFrame)\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Add the name of the DataFrame to df_names list\n",
    "        # If the DataFrame does not have a name attribute, use the function's name as a placeholder\n",
    "        name = getattr(result, 'name', func.__name__)\n",
    "        df_names.append(name)\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage:\n",
    "@track_dataframe\n",
    "def create_example_df():\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "    return df\n",
    "\n",
    "# Call the function to test the decorator\n",
    "example_df = create_example_df()\n",
    "\n",
    "# Display the df_names list\n",
    "# df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the extraction directory\n",
    "directory = \"data/\"\n",
    "files = os.listdir(directory)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = r'..raw_credits.csv'\n",
    "\n",
    "# df = pd.read_csv(csv_file_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titles_df = pd.read_csv(os.path.join(directory, 'raw_titles.csv'), index_col = \"index\")\n",
    "raw_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_credits_df = pd.read_csv(os.path.join(directory, 'raw_credits.csv'), index_col = \"index\")\n",
    "raw_credits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_titles_df missing values with %\n",
    "\"\"\"seasons (64.74% missing): This is expected, as the seasons column would primarily apply to TV shows, and not all entries in the dataset are TV shows.\n",
    "age_certification (44.95% missing): Almost half of the titles lack age certification data.\n",
    "imdb_votes (9.28% missing): A minor percentage of titles don't have the number of IMDB votes.\n",
    "imdb_score (9.01% missing): Similarly, a small percentage of titles don't have IMDB scores.\n",
    "imdb_id (7.65% missing): Some titles don't have their corresponding IMDB IDs.\n",
    "title (0.02% missing): Only one title is missing its name.\"\"\"\n",
    "compute_missing_values(raw_titles_df)\n",
    "# raw_titles_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_missing_values(raw_credits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_titles = DataCleaning(raw_titles_df)\n",
    "\n",
    "# df = processing.generate_factor_id(id_columns=['id'], id_field='new_id')\n",
    "processing_titles.remove_duplicates()\n",
    "processing_titles.remove_column_duplicates('id')\n",
    "processing_titles.remove_column_duplicates('imdb_id')\n",
    "# processing_titles.generate_factor_id(id_columns=['id'], id_field='id')\n",
    "# df = processing.replace_to_none()\n",
    "# df = add_id_from_index(df)\n",
    "raw_titles_df = processing_titles.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_credits = DataCleaning(raw_credits_df)\n",
    "\n",
    "# df = processing.generate_factor_id(id_columns=['id'], id_field='new_id')\n",
    "processing_credits.remove_duplicates()\n",
    "# df = processing.replace_to_none()\n",
    "# df = add_id_from_index(df)\n",
    "raw_credits_df = processing_credits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global list to store DataFrame names\n",
    "df_names = ['raw_titles_df', 'raw_credits_df']\n",
    "\n",
    "titles_columns_for_null = ['age_certification', 'seasons', 'imdb_id', 'genres', 'production_countries']\n",
    "\n",
    "titles_columns_for_median_or_mode = ['imdb_score', 'imdb_votes']\n",
    "\n",
    "titles_columns_for_brackets = ['genres', 'production_countries']\n",
    "\n",
    "credits_columns_for_null = ['name',\t'character',\t'role']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in titles_columns_for_brackets:\n",
    "    raw_titles_df[column] = processing_titles.remove_brackets_and_quotes(column_name=column)\n",
    "\n",
    "for column in titles_columns_for_null:\n",
    "    raw_titles_df[column] = processing_titles.missing_to_zero(column_name=column)\n",
    "\n",
    "for column in titles_columns_for_median_or_mode:\n",
    "    raw_titles_df[column] = processing_titles.missing_to_median_or_mode(column_name=column)\n",
    "\n",
    "raw_titles_df = raw_titles_df.dropna()\n",
    "\n",
    "raw_titles_df = raw_titles_df.copy()\n",
    "\n",
    "raw_titles_df[raw_titles_df['genres']==\"None\"]\n",
    "\n",
    "# for column in column_list_int:\n",
    "#     df[column] = processing.number_cleaning(column_name=column, dtype='int')\n",
    "\n",
    "# for column in column_list_float:\n",
    "\n",
    "#     df[column] = processing.number_cleaning(column_name=column, dtype='float')\n",
    "\n",
    "# converter = DateConverter()\n",
    "\n",
    "# for column in column_list_date:\n",
    "#     df[column] = converter.prepare_date(df[column])\n",
    "\n",
    "# spliting = ColumnProcessor(df)\n",
    "\n",
    "# for column in column_list_split:\n",
    "#     bridge_dfs[column], processed_dfs[column] = spliting.process_column_split(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in credits_columns_for_null:\n",
    "    raw_credits_df[column] = processing_credits.missing_to_zero(column_name=column)\n",
    "    \n",
    "raw_credits_df = raw_credits_df.copy()\n",
    "\n",
    "raw_credits_df[raw_credits_df['character']==\"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titles_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(raw_titles_df, pd.DataFrame):\n",
    "    print(\"df is a pandas DataFrame!\")\n",
    "else:\n",
    "    print(\"df is not a pandas DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"raw_titles_df.csv\"\n",
    "raw_titles_df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path = \"raw_credits_df.csv\"\n",
    "raw_credits_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_titles_df.csv\")\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mapping for column names and types.\"\"\"\n",
    "columns_dict = {\n",
    "                'id': 'string',\n",
    "                'title': 'string',\n",
    "                'type': 'string',\n",
    "                'release_year': 'int', # DATE YEAR? OR INT\n",
    "                'age_certification': 'string',\n",
    "                'runtime': 'int', \n",
    "                'genres': 'string', # Or list\n",
    "                'production_countries': 'string',# Or list\n",
    "                'seasons': 'int',\n",
    "                'imdb_id': 'string',\n",
    "                'imdb_score': 'float64',\n",
    "                'imdb_votes': 'int'\n",
    "                }\n",
    "\n",
    "errors = []\n",
    "\n",
    "for column, dtype in columns_dict.items():\n",
    "    try:\n",
    "        raw_titles_df[column] = raw_titles_df[column].astype(dtype)\n",
    "    except ValueError:\n",
    "        errors.append(f'Column {column} dtype change failed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titles_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_columns(raw_titles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric(raw_titles_df, 'imdb_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titles_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best rated movies\n",
    "raw_titles_df[raw_titles_df['type'] == 'MOVIE'].sort_values(by='imdb_score', ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract genres from the 'genres' column\n",
    "# all_genres = df['genres'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \").sum()\n",
    "all_genres = raw_titles_df['genres'].str.split(\", \").sum()\n",
    "\n",
    "# Convert the list of genres into a Series and count occurrences\n",
    "genre_counts = pd.Series(all_genres).value_counts()\n",
    "\n",
    "# Plot the top 10 genres\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.barplot(y=genre_counts.index[:10], x=genre_counts.values[:10], palette='magma')\n",
    "plt.title('Top 10 Genres on Netflix')\n",
    "plt.xlabel('Number of titles')\n",
    "plt.ylabel('Genre')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = raw_titles_df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "# categorical_cols = ['title', 'series', 'bookFormat', 'edition', 'publisher']\n",
    "categorical_cols = raw_titles_df.select_dtypes(include=['string', 'object']).columns.tolist()\n",
    "numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = raw_titles_df[numerical_cols].describe().T\n",
    "desc_stats[\"mode\"] = raw_titles_df[numerical_cols].mode().iloc[0]\n",
    "desc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create analyses dictionary to view for cateogrical columns. \"\"\"\n",
    "analysis_results_dict = {}\n",
    "non_str_columns = []\n",
    "\n",
    "for column in raw_titles_df.columns:\n",
    "    try:\n",
    "        results, shortest, longest = analyze_categorical_column(raw_titles_df, column)\n",
    "        analysis_results_dict[column] = {\n",
    "            \"analysis_results\": results,\n",
    "            \"shortest_value\": shortest,\n",
    "            \"longest_value\": longest\n",
    "        }\n",
    "    except Exception as e:\n",
    "        non_str_columns.append(column)\n",
    "\n",
    "non_str_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'type'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'title'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'age_certification'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'production_countries'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'genres'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = 'imdb_id'\n",
    "analysis_results_dict[new_col]['analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_max_lengths(raw_titles_df, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computing correlation matrix between numerical columns and plotting it \n",
    "for ease of visualization. Some columns are highly correlated with one another, \n",
    "which makes sense because a votes and ratings are associated.\n",
    "\"\"\"\n",
    "corr_matrix = raw_titles_df[numerical_cols].corr().round(2)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, linewidths=.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting categorical Columns, first 10 values only.\"\"\"\n",
    "plt.figure(figsize=(15, 5 * len(categorical_cols)))\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    plt.subplot(len(categorical_cols), 1, i)\n",
    "    sns.countplot(data=raw_titles_df, y=col, order=raw_titles_df[col].value_counts().index[:10])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Graphical investigation of numerical columns for outliers.\"\"\"\n",
    "outliers_numerical_cols(raw_titles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"View distribution of numerical columns.\"\"\"\n",
    "distribution_numerical_cols(raw_titles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The dataset predominantly contains titles released from the early 2000s onwards, with a peak around the 2010s. There are fewer titles from the mid-20th century, and the number increases gradually from the 1980s.\n",
    "\n",
    "Above statistical analsyis outlines vital information of dataset:\n",
    "\n",
    "Netflix is streaming both old and latest released movies and TV Shows\n",
    "Oldest Movie was released in the Year 1954 whereas oldest TV show was released in the year 1969\n",
    "Latest Movie and TV show steaming on Netflix were both released in the Year 2022\n",
    "IMDB_SCORE ranges from 6.5 to 9.5 for both Movies and TV shows\n",
    "One particular movie has gained a skyscrapper votes of more than 20 lakhs\n",
    "One particular TV show made a splash with more than 17 lakhs votes\n",
    "Maximum time duration for a Movie and a TV show are 229 and 141 minutes respectively\n",
    "Particular TV show has released 21 season which depicts its huge acceptence across the world\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplots illustrates that:  \n",
    "Top Movies and TV shows are largely produced by United States  \n",
    "India is the second contributor in producing top highly rated movies followed by Great Britain  \n",
    "TV shows produced by Great Britain and Japan are almost same but are far behind than US  \n",
    "US turned out to be the great player in an Entertainment Industry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = raw_credits_df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "categorical_cols = raw_credits_df.select_dtypes(include=['string', 'object']).columns.tolist()\n",
    "numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting categorical Columns, first 10 values only.\"\"\"\n",
    "plt.figure(figsize=(15, 5 * len(categorical_cols)))\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    plt.subplot(len(categorical_cols), 1, i)\n",
    "    sns.countplot(data=raw_credits_df, y=col, order=raw_credits_df[col].value_counts().index[:10])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_titles = raw_credits_df[['person_id','id']].duplicated(keep=False)\n",
    "raw_credits_df[duplicate_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's split the 'genres' column into a list of genres\n",
    "exploded_df = raw_titles_df.copy()\n",
    "exploded_df['genres'] = exploded_df['genres'].str.split(', ')\n",
    "exploded_df = exploded_df.explode('genres')\n",
    "\n",
    "# Now, we can group by individual genre and calculate the mean for 'imdb_score' and 'imdb_votes'\n",
    "score_genre = exploded_df.groupby('genres')[['imdb_score','imdb_votes']].mean().reset_index()\n",
    "\n",
    "# Sort by 'imdb_votes' in descending order\n",
    "score_genre_sorted = score_genre.sort_values(by=['imdb_votes'], ascending = False)\n",
    "# score_genre_sorted = score_genre.sort_values(by=['imdb_score'], ascending = False)\n",
    "score_genre_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Here are the top 5 genres based on the average number of IMDb votes:\n",
    "\n",
    "  Western: Avg IMDb Score = 6.60, Avg IMDb Votes = 96,100\n",
    "  War: Avg IMDb Score = 7.08, Avg IMDb Votes = 53,060\n",
    "  Sci-Fi: Avg IMDb Score = 6.58, Avg IMDb Votes = 46,177\n",
    "  Thriller: Avg IMDb Score = 6.37, Avg IMDb Votes = 45,487\n",
    "  Horror: Avg IMDb Score = 6.03, Avg IMDb Votes = 43,547\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a barplot for genres by IMDb score\n",
    "sns.barplot(x='imdb_score', y='genres', data=score_genre_sorted, palette='viridis')\n",
    "plt.title('Average IMDb Score by Genre', fontsize=16)\n",
    "plt.xlabel('Average IMDb Score', fontsize=14)\n",
    "plt.ylabel('Genre', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure for the second plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x='imdb_votes', y='genres', data=score_genre_sorted, palette='viridis')\n",
    "plt.title('Average IMDb Votes by Genre', fontsize=16)\n",
    "plt.xlabel('Average IMDb Votes', fontsize=14)\n",
    "plt.ylabel('Genre', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_id(df: pd.DataFrame, *args, id_field: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a unique serialized integer value for each unique combination of\n",
    "    specified columns using factorize.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        *args: Columns to be used for unique combinations.\n",
    "        id_field (str): Name of the new ID field.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new ID field.\n",
    "    \"\"\"\n",
    "\n",
    "    combined = df[list(args)].astype(str).agg(\"_\".join, axis=1)\n",
    "\n",
    "    df[id_field] = pd.factorize(combined)[0]+1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = raw_titles_df[['id', 'title', 'release_year', 'age_certification', 'runtime', 'imdb_id', 'imdb_score', 'imdb_votes']]\n",
    "\n",
    "title_type_df = raw_titles_df[['id', 'type']].copy()\n",
    "title_type_df = factorize_id(title_type_df, 'type', id_field='type_id')\n",
    "\n",
    "\n",
    "title_genres_df = raw_titles_df[['id', 'genres']].copy()\n",
    "title_genres_df['genres'] = title_genres_df['genres'].str.split(\", \")\n",
    "title_genres_df = title_genres_df.explode('genres')\n",
    "title_genres_df = factorize_id(title_genres_df, 'genres', id_field='genre_id')\n",
    "title_genres_df = title_genres_df.rename(columns={'genres': 'genre'})\n",
    "\n",
    "\n",
    "title_production_countries_df = raw_titles_df[['id', 'production_countries']].copy()\n",
    "title_production_countries_df['production_countries'] = title_production_countries_df['production_countries'].str.split(\", \")\n",
    "title_production_countries_df = title_production_countries_df.explode('production_countries')\n",
    "title_production_countries_df = factorize_id(title_production_countries_df, 'production_countries', id_field='production_id')\n",
    "title_production_countries_df = title_production_countries_df.rename(columns={'production_countries': 'country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create titles Table\n",
    "titles_df = raw_titles_df[['id', 'title', 'release_year', 'age_certification', 'runtime', 'imdb_id', 'imdb_score', 'imdb_votes']]\n",
    "# titles_df = titles_df.rename(columns={'id': 'id'})\n",
    "\n",
    "# Create title Type Table\n",
    "title_type_df = raw_titles_df[['id', 'type']].copy()\n",
    "# title_type_df['type_id'] = title_type_df.index + 1  # Assigning a unique ID for each type entry\n",
    "title_type_df = factorize_id(title_type_df, 'type', id_field='type_id')\n",
    "# title_type_df = title_type_df.rename(columns={'id': 'id'})\n",
    "\n",
    "# Create title Genres Table\n",
    "title_genres_df = raw_titles_df[['id', 'genres']].copy()\n",
    "# title_genres_df['genres'] = title_genres_df['genres'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "title_genres_df['genres'] = title_genres_df['genres'].str.split(\", \")\n",
    "title_genres_df = title_genres_df.explode('genres')\n",
    "# title_genres_df['genre_id'] = title_genres_df.index + 1  # Assigning a unique ID for each genre entry\n",
    "title_genres_df = factorize_id(title_genres_df, 'genres', id_field='genre_id')\n",
    "title_genres_df = title_genres_df.rename(columns={'genres': 'genre'})\n",
    "# title_genres_df = title_genres_df.rename(columns={'genres': 'genre'})\n",
    "\n",
    "# Create title Production Countries Table\n",
    "title_production_countries_df = raw_titles_df[['id', 'production_countries']].copy()\n",
    "# title_production_countries_df['production_countries'] = title_production_countries_df['production_countries'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "title_production_countries_df['production_countries'] = title_production_countries_df['production_countries'].str.split(\", \")\n",
    "title_production_countries_df = title_production_countries_df.explode('production_countries')\n",
    "title_production_countries_df = factorize_id(title_production_countries_df, 'production_countries', id_field='production_id')\n",
    "# title_production_countries_df['production_id'] = title_production_countries_df.index + 1  # Assigning a unique ID for each production country entry\n",
    "title_production_countries_df = title_production_countries_df.rename(columns={'production_countries': 'country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID\n",
    "raw_credits_df = factorize_id(raw_credits_df, 'name', id_field='person_id')\n",
    "\n",
    "# Create the persons table\n",
    "credits_persons_df = raw_credits_df[['person_id', 'name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Create the credits table\n",
    "credits_df = raw_credits_df.copy()\n",
    "credits_df.drop(columns=['name'], inplace=True)\n",
    "\n",
    "credits_df['credit_id'] = credits_df.index + 1\n",
    "credits_df = credits_df[['credit_id', 'id', 'person_id', 'character', 'role']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving original\n",
    "# Create titles Table\n",
    "titles_df = raw_titles_df[['id', 'title', 'release_year', 'age_certification', 'runtime', 'imdb_id', 'imdb_score', 'imdb_votes']]\n",
    "titles_df = titles_df.rename(columns={'id': 'id'})\n",
    "\n",
    "# Create title Type Table\n",
    "title_type_df = raw_titles_df[['id', 'type']].copy()\n",
    "title_type_df['type_id'] = title_type_df.index + 1  # Assigning a unique ID for each type entry\n",
    "title_type_df = title_type_df.rename(columns={'id': 'id'})\n",
    "\n",
    "# Create title Genres Table\n",
    "title_genres_df = raw_titles_df[['id', 'genres']].copy()\n",
    "title_genres_df['genres'] = title_genres_df['genres'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "title_genres_df = title_genres_df.explode('genres')\n",
    "title_genres_df['genre_id'] = title_genres_df.index + 1  # Assigning a unique ID for each genre entry\n",
    "title_genres_df = title_genres_df.rename(columns={'id': 'id', 'genres': 'genre'})\n",
    "\n",
    "# Create title Production Countries Table\n",
    "title_production_countries_df = raw_titles_df[['id', 'production_countries']].copy()\n",
    "title_production_countries_df['production_countries'] = title_production_countries_df['production_countries'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "title_production_countries_df = title_production_countries_df.explode('production_countries')\n",
    "title_production_countries_df['production_id'] = title_production_countries_df.index + 1  # Assigning a unique ID for each production country entry\n",
    "title_production_countries_df = title_production_countries_df.rename(columns={'id': 'id', 'production_countries': 'country'})\n",
    "\n",
    "# Write out the normalized tables to CSV\n",
    "output_dir = \"/mnt/data/normalized_tables\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "titles_df.to_csv(os.path.join(output_dir, 'titles.csv'), index=False)\n",
    "title_type_df.to_csv(os.path.join(output_dir, 'title_type.csv'), index=False)\n",
    "title_genres_df.to_csv(os.path.join(output_dir, 'title_genres.csv'), index=False)\n",
    "title_production_countries_df.to_csv(os.path.join(output_dir, 'title_production_countries.csv'), index=False)\n",
    "\n",
    "output_dir\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_movie_by_year_df = lowercase_column_names(pd.read_csv(os.path.join(directory, 'Best Movie by Year Netflix.csv'), index_col = \"index\"))\n",
    "best_movies_df = lowercase_column_names(pd.read_csv(os.path.join(directory, 'Best Movies Netflix.csv'), index_col = \"index\"))\n",
    "best_shows_df = lowercase_column_names(pd.read_csv(os.path.join(directory, 'Best Shows Netflix.csv'), index_col = \"index\"))\n",
    "best_show_by_year_df = lowercase_column_names(pd.read_csv(os.path.join(directory, 'Best Show by Year Netflix.csv'), index_col = \"index\"))\n",
    "\n",
    "# best_movie_by_year_df = pd.read_csv(os.path.join(directory, 'Best Movie by Year Netflix.csv'), index_col = \"index\")\n",
    "# best_movies_df = pd.read_csv(os.path.join(directory, 'Best Movies Netflix.csv'), index_col = \"index\")\n",
    "# best_shows_df = pd.read_csv(os.path.join(directory, 'Best Shows Netflix.csv'), index_col = \"index\")\n",
    "# best_show_by_year_df = pd.read_csv(os.path.join(directory, 'Best Show by Year Netflix.csv'), index_col = \"index\")\n",
    "\n",
    "# Converting column names to lower case\n",
    "# best_movie_by_year_df.columns = [c.lower() for c in best_movie_by_year_df.columns]\n",
    "# best_movies_df.columns = [c.lower() for c in best_movies_df.columns]\n",
    "# best_shows_df.columns = [c.lower() for c in best_shows_df.columns]\n",
    "# best_show_by_year_df.columns = [c.lower() for c in best_show_by_year_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this dataset with the titles_df based on title and release year\n",
    "best_movie_by_year_merged = pd.merge(titles_df, best_movie_by_year_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n",
    "best_movies_merged = pd.merge(titles_df, best_movies_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n",
    "best_show_by_year_merged = pd.merge(titles_df, best_show_by_year_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n",
    "best_shows_merged = pd.merge(titles_df, best_shows_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_movie_by_year_merged['title'].count() == best_movie_by_year_df['title'].count())\n",
    "print(best_movies_merged['title'].count() == best_movies_df['title'].count())\n",
    "print(best_show_by_year_merged['title'].count() == best_show_by_year_df['title'].count())\n",
    "print(best_shows_merged['title'].count() == best_shows_df['title'].count(), \"\\nDifference between: \", best_shows_merged['title'].count() - best_shows_df['title'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Additional Tables/Flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For each of the merged datasets (best_movie_by_year_merged, best_movies_merged, best_show_by_year_merged, and best_shows_merged), we'll create a flag in the titles_df dataframe. If a title is present in one of these datasets, its corresponding flag will be set to 1; otherwise, it will be 0.\n",
    "We'll create the following flags:\n",
    "is_best_movie_by_year\n",
    "is_best_movie\n",
    "is_best_show_by_year\n",
    "is_best_show\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize flags in titles_df\n",
    "titles_df['is_best_movie_by_year'] = 0\n",
    "titles_df['is_best_movie'] = 0\n",
    "titles_df['is_best_show_by_year'] = 0\n",
    "titles_df['is_best_show'] = 0\n",
    "\n",
    "# Set the flags based on the presence of id in the merged datasets\n",
    "titles_df.loc[titles_df['id'].isin(best_movie_by_year_merged['id']), 'is_best_movie_by_year'] = 1\n",
    "titles_df.loc[titles_df['id'].isin(best_movies_merged['id']), 'is_best_movie'] = 1\n",
    "titles_df.loc[titles_df['id'].isin(best_show_by_year_merged['id']), 'is_best_show_by_year'] = 1\n",
    "titles_df.loc[titles_df['id'].isin(best_shows_merged['id']), 'is_best_show'] = 1\n",
    "\n",
    "# Display the titles_df with the new flags\n",
    "titles_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Consistency Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Identify Unmatched titles:\n",
    "For each \"best\" dataset, find titles that didn't find a match in the primary dataset (titles_df).\n",
    "Flag Discrepancies:\n",
    "Add a column in each \"best\" dataset to indicate if the title was matched in the primary dataset or not.\n",
    "Review & Address Discrepancies:\n",
    "List out the unmatched titles for further review.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store unmatched titles for each \"best\" dataset\n",
    "unmatched_titles = {\n",
    "    'best_movie_by_year': [],\n",
    "    'best_movies': [],\n",
    "    'best_show_by_year': [],\n",
    "    'best_shows': []\n",
    "}\n",
    "\n",
    "# Identify Unmatched titles for \"Best Movie by Year Netflix.csv\"\n",
    "unmatched_titles['best_movie_by_year'] = best_movie_by_year_df.loc[~best_movie_by_year_df['title'].isin(best_movie_by_year_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_movies'] = best_movies_df.loc[~best_movies_df['title'].isin(best_movies_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_show_by_year'] = best_show_by_year_df.loc[~best_show_by_year_df['title'].isin(best_show_by_year_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_shows'] = best_shows_df.loc[~best_shows_df['title'].isin(best_shows_merged['title']), 'title'].tolist()\n",
    "\n",
    "unmatched_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT! \n",
    "# Re-load the necessary datasets for consistency checks\n",
    "best_movie_by_year_df = pd.read_csv(os.path.join(directory, 'Best Movie by Year Netflix.csv'))\n",
    "best_movies_df = pd.read_csv(os.path.join(directory, 'Best Movies Netflix.csv'))\n",
    "best_show_by_year_df = pd.read_csv(os.path.join(directory, 'Best Show by Year Netflix.csv'))\n",
    "best_shows_df = pd.read_csv(os.path.join(directory, 'Best Shows Netflix.csv'))\n",
    "\n",
    "# Re-merge the datasets\n",
    "best_movie_by_year_merged = pd.merge(titles_df, best_movie_by_year_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n",
    "best_movies_merged = pd.merge(titles_df, best_movies_df, how='inner', left_on='title', right_on='title')\n",
    "best_show_by_year_merged = pd.merge(titles_df, best_show_by_year_df, how='inner', left_on=['title', 'release_year'], right_on=['title', 'release_year'])\n",
    "best_shows_merged = pd.merge(titles_df, best_shows_df, how='inner', left_on='title', right_on='title')\n",
    "\n",
    "# Initialize dictionaries to store unmatched titles for each \"best\" dataset\n",
    "unmatched_titles = {\n",
    "    'best_movie_by_year': [],\n",
    "    'best_movies': [],\n",
    "    'best_show_by_year': [],\n",
    "    'best_shows': []\n",
    "}\n",
    "\n",
    "# Find unmatched titles\n",
    "unmatched_titles['best_movie_by_year'] = best_movie_by_year_df.loc[~best_movie_by_year_df['title'].isin(best_movie_by_year_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_movies'] = best_movies_df.loc[~best_movies_df['title'].isin(best_movies_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_show_by_year'] = best_show_by_year_df.loc[~best_show_by_year_df['title'].isin(best_show_by_year_merged['title']), 'title'].tolist()\n",
    "unmatched_titles['best_shows'] = best_shows_df.loc[~best_shows_df['title'].isin(best_shows_merged['title']), 'title'].tolist()\n",
    "\n",
    "unmatched_titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great news! The consistency checks have been completed successfully, and it appears that all titles in the \"best\" datasets have been matched with the titles in the primary dataset (titles_df). There are no unmatched titles.\n",
    "\n",
    "This means that the titles in the \"best\" datasets are consistent with the primary dataset, ensuring data integrity.\n",
    "\n",
    "Would you like to proceed with any other tasks or actions related to these datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all ids in credits_df are present in titles_df\n",
    "missing_ids = set(credits_df['id']) - set(titles_df['id'])\n",
    "if missing_ids:\n",
    "    print(f\"Missing ids: {missing_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['id'] == 'tm1063792']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits_df[credits_df['id'] == 'tm1063792']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop_titles = titles_df[titles_df['id'] == missing_ids].index\n",
    "titles_df.drop(index_to_drop_titles, inplace=True)\n",
    "index_to_drop_credits = credits_df[credits_df['id'] == missing_ids].index\n",
    "credits_df.drop(index_to_drop_credits, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the normalized and enriched tables to CSV\n",
    "output_dir = \"normalized_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "titles_df.to_csv(os.path.join(output_dir, 'titles.csv'), index=False)\n",
    "title_type_df.to_csv(os.path.join(output_dir, 'title_type.csv'), index=False)\n",
    "title_genres_df.to_csv(os.path.join(output_dir, 'title_genres.csv'), index=False)\n",
    "title_production_countries_df.to_csv(os.path.join(output_dir, 'title_production_countries.csv'), index=False)\n",
    "\n",
    "credits_df.to_csv(os.path.join(output_dir, 'credits.csv'), index=False)\n",
    "credits_persons_df.to_csv(os.path.join(output_dir, 'credits_persons.csv'), index=False)\n",
    "\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "output_dir = \"normalized_tables/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "titles_df = pd.read_csv(os.path.join(output_dir, 'titles.csv'))\n",
    "title_type_df = pd.read_csv(os.path.join(output_dir, 'title_type.csv'))\n",
    "title_genres_df = pd.read_csv(os.path.join(output_dir, 'title_genres.csv')) \n",
    "title_production_countries_df = pd.read_csv(os.path.join(output_dir, 'title_production_countries.csv'))\n",
    "credits_df = pd.read_csv(os.path.join(output_dir, 'credits.csv'))\n",
    "credits_persons_df = pd.read_csv(os.path.join(output_dir, 'credits_persons.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Imported connection data used to connect to MySQL without. \n",
    "# Then creating a database 'netflix_movies' and renewing engine to include database.\"\"\"\n",
    "# engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}:{port}/\")\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS {database}\"))\n",
    "# conn.close()\n",
    "\n",
    "# engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}:{port}/{database}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_col_max_lengths(titles_df)\n",
    "df_col_max_lengths(title_type_df)\n",
    "# df_col_max_lengths(title_genres_df)\n",
    "# df_col_max_lengths(title_production_countries_df)\n",
    "# df_col_max_lengths(credits_df)\n",
    "# df_col_max_lengths(credits_persons_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Missing Values  Percentage (%)\n",
       "person_id               0             0.0\n",
       "name                    0             0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute_missing_values(titles_df)\n",
    "# compute_missing_values(title_type_df)\n",
    "# compute_missing_values(title_genres_df)\n",
    "# compute_missing_values(title_production_countries_df)\n",
    "# compute_missing_values(credits_df)\n",
    "compute_missing_values(credits_persons_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trials and errors\n",
    "# \"\"\"Create SQLAlchemy classes representing MySQL database tables.\"\"\"\n",
    "# from sqlalchemy import Column, Integer, String, Float, Boolean, ForeignKey\n",
    "# from sqlalchemy.orm import relationship\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "# from sqlalchemy import PrimaryKeyConstraint\n",
    "# from sqlalchemy import create_engine, text\n",
    "\n",
    "# Base = declarative_base()\n",
    "\n",
    "# @track_tables\n",
    "# class Titles(Base):\n",
    "#     __tablename__ = 'titles'\n",
    "    \n",
    "#     id = Column(String(20), primary_key=True)\n",
    "#     title = Column(String(300), nullable=False)\n",
    "#     release_year = Column(Integer)\n",
    "#     age_certification = Column(String(10))\n",
    "#     runtime = Column(Integer)\n",
    "#     imdb_id = Column(String(20))\n",
    "#     imdb_score = Column(Float)\n",
    "#     imdb_votes = Column(Integer)\n",
    "#     is_best_movie_by_year = Column(Boolean)\n",
    "#     is_best_movie = Column(Boolean)\n",
    "#     is_best_show_by_year = Column(Boolean)\n",
    "#     is_best_show = Column(Boolean)\n",
    "\n",
    "#     title_type = relationship(\"TitleType\", uselist=False, back_populates=\"titles\")\n",
    "#     title_genres = relationship(\"TitleGenres\", back_populates=\"titles\")\n",
    "#     title_production_countries = relationship(\"TitleProductionCountries\", back_populates=\"titles\")\n",
    "#     # credits = relationship(\"Credits\", back_populates=\"titles\")\n",
    "\n",
    "# @track_tables\n",
    "# class TitleType(Base):\n",
    "#     __tablename__ = 'title_type'\n",
    "    \n",
    "#     type_id = Column(Integer, autoincrement=True)\n",
    "#     id = Column(String(20), ForeignKey('titles.id'))\n",
    "#     type = Column(String(50))\n",
    "    \n",
    "#     __table_args__ = (\n",
    "#         PrimaryKeyConstraint('type_id', 'id'),\n",
    "#     )\n",
    "\n",
    "#     titles = relationship(\"Titles\", back_populates=\"title_type\")\n",
    "\n",
    "# @track_tables\n",
    "# class TitleGenres(Base):\n",
    "#     __tablename__ = 'title_genres'\n",
    "    \n",
    "#     genre_id = Column(Integer, autoincrement=True)\n",
    "#     id = Column(String(20), ForeignKey('titles.id'))\n",
    "#     genre = Column(String(50))\n",
    "    \n",
    "#     __table_args__ = (\n",
    "#         PrimaryKeyConstraint('genre_id', 'id'),\n",
    "#     )\n",
    "    \n",
    "#     titles = relationship(\"Titles\", back_populates=\"title_genres\")\n",
    "\n",
    "# @track_tables\n",
    "# class TitleProductionCountries(Base):\n",
    "#     __tablename__ = 'title_production_countries'\n",
    "    \n",
    "#     production_id = Column(Integer, autoincrement=True)\n",
    "#     id = Column(String(20), ForeignKey('titles.id'))\n",
    "#     country = Column(String(50))\n",
    "    \n",
    "#     __table_args__ = (\n",
    "#         PrimaryKeyConstraint('production_id', 'id'),\n",
    "#     )\n",
    "    \n",
    "#     titles = relationship(\"Titles\", back_populates=\"title_production_countries\")\n",
    "\n",
    "# @track_tables\n",
    "# class Persons(Base):\n",
    "#     __tablename__ = 'persons'\n",
    "    \n",
    "#     person_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "#     name = Column(String(300), nullable=False)\n",
    "    \n",
    "#     credits = relationship(\"Credits\", back_populates=\"persons\")\n",
    "\n",
    "# @track_tables\n",
    "# class Credits(Base):\n",
    "#     __tablename__ = 'credits'\n",
    "    \n",
    "#     credit_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "#     # credit_id = Column(Integer)#, primary_key=True, autoincrement=True)\n",
    "#     # id = Column(String(20), ForeignKey('titles.id'))\n",
    "#     id = Column(String(20))#, ForeignKey('titles.id'))\n",
    "#     person_id = Column(Integer, ForeignKey('persons.person_id'))\n",
    "#     character = Column(String(300))\n",
    "#     role = Column(String(50))\n",
    "    \n",
    "#     __table_args__ = (\n",
    "#         PrimaryKeyConstraint('credit_id', 'id'),\n",
    "#     )\n",
    "    \n",
    "#     # titles = relationship(\"Titles\", back_populates=\"credits\")\n",
    "#     persons = relationship(\"Persons\", back_populates=\"credits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base = declarative_base()\n",
    "\n",
    "@track_tables\n",
    "class Titles(Base):\n",
    "    __tablename__ = 'titles'\n",
    "    \n",
    "    id = Column(String(20), primary_key=True)\n",
    "    title = Column(String(300), nullable=False)\n",
    "    release_year = Column(Integer)\n",
    "    age_certification = Column(String(10))\n",
    "    runtime = Column(Integer)\n",
    "    imdb_id = Column(String(20))\n",
    "    imdb_score = Column(Float)\n",
    "    imdb_votes = Column(Integer)\n",
    "    is_best_movie_by_year = Column(Boolean)\n",
    "    is_best_movie = Column(Boolean)\n",
    "    is_best_show_by_year = Column(Boolean)\n",
    "    is_best_show = Column(Boolean)\n",
    "\n",
    "    title_type = relationship(\"TitleType\", uselist=False, back_populates=\"titles\")\n",
    "    title_genres = relationship(\"TitleGenres\", back_populates=\"titles\")\n",
    "    title_production_countries = relationship(\"TitleProductionCountries\", back_populates=\"titles\")\n",
    "    credits = relationship(\"Credits\", back_populates=\"titles\")\n",
    "\n",
    "@track_tables\n",
    "class TitleType(Base):\n",
    "    __tablename__ = 'title_type'\n",
    "    \n",
    "    type_id = Column(Integer, autoincrement=True)\n",
    "    id = Column(String(20), ForeignKey('titles.id'))\n",
    "    type = Column(String(50))\n",
    "    \n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('type_id', 'id'),\n",
    "    )\n",
    "\n",
    "    titles = relationship(\"Titles\", back_populates=\"title_type\")\n",
    "\n",
    "@track_tables\n",
    "class TitleGenres(Base):\n",
    "    __tablename__ = 'title_genres'\n",
    "    \n",
    "    genre_id = Column(Integer, autoincrement=True)\n",
    "    id = Column(String(20), ForeignKey('titles.id'))\n",
    "    genre = Column(String(50))\n",
    "    \n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('genre_id', 'id'),\n",
    "    )\n",
    "    \n",
    "    titles = relationship(\"Titles\", back_populates=\"title_genres\")\n",
    "\n",
    "@track_tables\n",
    "class TitleProductionCountries(Base):\n",
    "    __tablename__ = 'title_production_countries'\n",
    "    \n",
    "    production_id = Column(Integer, autoincrement=True)\n",
    "    id = Column(String(20), ForeignKey('titles.id'))\n",
    "    country = Column(String(50))\n",
    "    \n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('production_id', 'id'),\n",
    "    )\n",
    "    \n",
    "    titles = relationship(\"Titles\", back_populates=\"title_production_countries\")\n",
    "\n",
    "@track_tables\n",
    "class Persons(Base):\n",
    "    __tablename__ = 'persons'\n",
    "    \n",
    "    person_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String(300), nullable=False)\n",
    "    \n",
    "    credits = relationship(\"Credits\", back_populates=\"persons\")\n",
    "\n",
    "@track_tables\n",
    "class Credits(Base):\n",
    "    __tablename__ = 'credits'\n",
    "    \n",
    "    credit_id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    # credit_id = Column(Integer)#, primary_key=True, autoincrement=True)\n",
    "    id = Column(String(20), ForeignKey('titles.id'))\n",
    "    # id = Column(String(20))#, ForeignKey('titles.id'))\n",
    "    person_id = Column(Integer, ForeignKey('persons.person_id'))\n",
    "    character = Column(String(300))\n",
    "    role = Column(String(50))\n",
    "    \n",
    "    # __table_args__ = (\n",
    "    #     PrimaryKeyConstraint('credit_id', 'id'),\n",
    "    # )\n",
    "    \n",
    "    titles = relationship(\"Titles\", back_populates=\"credits\")\n",
    "    persons = relationship(\"Persons\", back_populates=\"credits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create all tables in MySQL database.\"\"\"\n",
    "# Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base.metadata.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['titles',\n",
       " 'title_type',\n",
       " 'title_genres',\n",
       " 'title_production_countries',\n",
       " 'persons',\n",
       " 'credits']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create the dfs_list that contains all dataframes, then table_names list that contain respective table names in MySQL database.\"\"\"\n",
    "db_tables = []\n",
    "t1 = Titles()\n",
    "t2 = TitleType()\n",
    "t3 = TitleGenres()\n",
    "t4 = TitleProductionCountries()\n",
    "t5 = Persons()\n",
    "t6 = Credits()\n",
    "db_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_tables = [\n",
    "#  'titles', 'title_type',\n",
    "#  'title_genres', 'title_production_countries',\n",
    "#  'persons', 'credits'\n",
    "#  ]\n",
    "\n",
    "dfs_list = [\n",
    "    titles_df, title_type_df, \n",
    "    title_genres_df, title_production_countries_df,\n",
    "    credits_persons_df, credits_df\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Turn off foreign key checks\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(\"SET FOREIGN_KEY_CHECKS=0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Upload all the data from the dataframes to MySQL database tables.\"\"\"\n",
    "# for df, db_table in zip(dfs_list, db_tables):\n",
    "#     print(f\"Inserting into table: {db_table}\")\n",
    "#     df.to_sql(\n",
    "#         name=db_table,\n",
    "#         con=engine,\n",
    "#         if_exists='append',\n",
    "#         index=False\n",
    "#     )\n",
    "    \n",
    "# print(\"Upload complete.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Turn foreign key checks back on\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(\"SET FOREIGN_KEY_CHECKS=1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_send = credits_df\n",
    "# table_name_to_ingest = 'credits'\n",
    "\n",
    "# df_to_send.to_sql(\n",
    "#         name=table_name_to_ingest,\n",
    "#         con=engine,\n",
    "#         if_exists='append',\n",
    "#         index=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Imported Class\n",
    "\"\"\"Create database connection and database.\"\"\"\n",
    "db_loader = DatabaseLoader()\n",
    "db_loader.create_engine()\n",
    "\n",
    "\"\"\"Create database and update engine.\"\"\"\n",
    "db_loader.create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create all tables in MySQL database.\"\"\"\n",
    "# db_loader.create_all()\n",
    "Base.metadata.create_all(db_loader.engine)\n",
    "\n",
    "\"\"\"Turn off foregin key constaint checks.\"\"\"\n",
    "db_loader.turn_off_fk_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Upload all the data from the dataframes to MySQL database tables.\"\"\"\n",
    "for df, db_table in zip(dfs_list, db_tables):\n",
    "    print(f\"Inserting into table: {db_table}\")\n",
    "    db_loader.send_data(df, db_table)\n",
    "    \n",
    "print(\"Upload complete.\")    \n",
    "\n",
    "\"\"\"Turn on foreign key checks.\"\"\"\n",
    "db_loader.turn_on_fk_check()\n",
    "\n",
    "\"\"\"Clear residual metadata.\"\"\"\n",
    "# Base.metadata.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Obtaining dependency information for flask from https://files.pythonhosted.org/packages/fd/56/26f0be8adc2b4257df20c1c4260ddd0aa396cf8e75d90ab2f7ff99bc34f9/flask-2.3.3-py3-none-any.whl.metadata\n",
      "  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting Werkzeug>=2.3.7 (from flask)\n",
      "  Obtaining dependency information for Werkzeug>=2.3.7 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Jinja2>=3.1.2 (from flask)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting itsdangerous>=2.1.2 (from flask)\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.1.3 (from flask)\n",
      "  Obtaining dependency information for click>=8.1.3 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting blinker>=1.6.2 (from flask)\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->flask)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/fe/09/c31503cb8150cf688c1534a7135cc39bb9092f8e0e6369ec73494d16ee0e/MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Downloading flask-2.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl (17 kB)\n",
      "Installing collected packages: MarkupSafe, itsdangerous, click, blinker, Werkzeug, Jinja2, flask\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.3 Werkzeug-2.3.7 blinker-1.6.2 click-8.1.7 flask-2.3.3 itsdangerous-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import FLask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "  return \"Home\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: fastapi[all]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install fastapi[all] uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(mysql+mysqlconnector://root:***@localhost:3306/netflix_movies)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = db_loader.engine\n",
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sqlalchemy import MetaData, Table\n",
    "metadata = MetaData()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends, Request\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Boolean, Float, MetaData, Table, Text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "import sqlparse\n",
    "\n",
    "# DATABASE_URL = \"mysql+mysqlconnector://username:password@localhost/netflix_movies\"\n",
    "# engine = create_engine(DATABASE_URL)\n",
    "\n",
    "metadata = MetaData()\n",
    "# Base = declarative_base()\n",
    "\n",
    "engine = db_loader.engine\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "@app.post(\"/execute_query/\")\n",
    "def execute_query(request: Request, db: SessionLocal = Depends(get_db)):\n",
    "    raw_query = await request.body()\n",
    "    try:\n",
    "        result = db.execute(text(raw_query.decode())).fetchall()\n",
    "        return {\"result\": [dict(row) for row in result]}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start API:\n",
    "# Replace <filename> with filename\n",
    "uvicorn filename:app --reload\n",
    "\n",
    "# Access:\n",
    "# http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/add_title/\")\n",
    "def add_title(title: Titles, db: Session = Depends(SessionLocal)):\n",
    "    db.add(title)\n",
    "    db.commit()\n",
    "    return {\"message\": \"Title added successfully\"}\n",
    "\n",
    "@app.post(\"/execute_query/\")\n",
    "async def execute_query(query: str, db: Session = Depends(SessionLocal)):\n",
    "    parsed = sqlparse.parse(query)\n",
    "    statement = parsed[0]\n",
    "\n",
    "    # Only allow SELECT statements for raw SQL\n",
    "    if statement.get_type() != \"SELECT\":\n",
    "        raise HTTPException(status_code=400, detail=\"Only SELECT statements are allowed for raw SQL\")\n",
    "\n",
    "    try:\n",
    "        result = db.execute(query)\n",
    "        return {\"data\": [dict(row) for row in result]}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/execute_query/\")\n",
    "async def execute_query(query: str):\n",
    "    # Parse the SQL statement using sqlparse\n",
    "    parsed = sqlparse.parse(query)\n",
    "    statement = parsed[0]\n",
    "    \n",
    "    # If it's an insert statement, validate\n",
    "    if statement.get_type() == \"INSERT\":\n",
    "        table_name = statement.get_tables()[0]\n",
    "        table = Table(table_name, metadata, autoload_with=engine)\n",
    "        \n",
    "        # Extract columns from the query\n",
    "        columns = [str(item) for item in statement.columns]\n",
    "        \n",
    "        # Check if all columns exist in the table and have the correct type\n",
    "        for column in columns:\n",
    "            if column not in table.c:\n",
    "                raise HTTPException(status_code=400, detail=f\"Column {column} doesn't exist in {table_name}.\")\n",
    "            \n",
    "            # You can add more checks here, e.g., for data types\n",
    "        \n",
    "    # If everything is fine, execute the query\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(query)\n",
    "            return {\"data\": [dict(row) for row in result]}\n",
    "    except SQLAlchemyError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "DATABASE_URL = \"mysql+mysqlconnector://user:password@localhost:3306/mydatabase\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "\n",
    "# Inserting a new record\n",
    "def insert_record(orm_class, data: dict):\n",
    "    with SessionLocal() as session:\n",
    "        record = orm_class(**data)\n",
    "        session.add(record)\n",
    "        session.commit()\n",
    "\n",
    "# Updating an existing record based on primary key\n",
    "def update_record(orm_class, primary_key_value: str, data: dict):\n",
    "    with SessionLocal() as session:\n",
    "        try:\n",
    "            record = session.query(orm_class).filter(orm_class.id == primary_key_value).one()\n",
    "            for key, value in data.items():\n",
    "                setattr(record, key, value)\n",
    "            session.commit()\n",
    "        except NoResultFound:\n",
    "            print(f\"No record found with ID: {primary_key_value}\")\n",
    "\n",
    "# Deleting an existing record based on primary key\n",
    "def delete_record(orm_class, primary_key_value: str):\n",
    "    with SessionLocal() as session:\n",
    "        try:\n",
    "            record = session.query(orm_class).filter(orm_class.id == primary_key_value).one()\n",
    "            session.delete(record)\n",
    "            session.commit()\n",
    "        except NoResultFound:\n",
    "            print(f\"No record found with ID: {primary_key_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# Insert a new title\n",
    "insert_record(Titles, {\"id\": \"some_id\", \"title\": \"Example Title\", ...})\n",
    "\n",
    "# Update a title\n",
    "update_record(Titles, \"some_id\", {\"title\": \"Updated Title\"})\n",
    "\n",
    "# Delete a title\n",
    "delete_record(Titles, \"some_id\")\n",
    "\n",
    "# Insert a new person\n",
    "insert_record(Persons, {\"person_id\": 123, \"name\": \"John Doe\"})\n",
    "\n",
    "# ... and so on for other ORM classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Insights:\n",
    "Yearly Trend of Best Movies and Shows:  \n",
    "Visualize the number of best movies and shows released each year.  \n",
    "Distribution of IMDb Scores:  \n",
    "Check the distribution of IMDb scores for best movies and best shows.  \n",
    "Compare the IMDb scores of the best movies and shows with the overall catalog.  \n",
    "Runtime Analysis:  \n",
    "Analyze the runtime distribution of best movies and shows. Are longer movies or shorter shows preferred?  \n",
    "Age Certification Analysis:  \n",
    "Analyze the distribution of age certifications for best movies and shows.  \n",
    "Release Year Distribution:  \n",
    "Visualize the distribution of release years for best movies and shows. This can show if newer content or older classics dominate the \"best\" lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot yearly trend for best movies and shows\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Best Movies by Year\n",
    "sns.countplot(data=best_movie_by_year_merged, x=\"release_year\", ax=ax[0], color='blue', order=best_movie_by_year_merged['release_year'].value_counts().index)\n",
    "ax[0].set_title(\"Yearly Trend of Best Movies on Netflix\")\n",
    "ax[0].set_xlabel(\"Release Year\")\n",
    "ax[0].set_ylabel(\"Number of Best Movies\")\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Best Shows by Year\n",
    "sns.countplot(data=best_show_by_year_merged, x=\"release_year\", ax=ax[1], color='green', order=best_show_by_year_merged['release_year'].value_counts().index)\n",
    "ax[1].set_title(\"Yearly Trend of Best Shows on Netflix\")\n",
    "ax[1].set_xlabel(\"Release Year\")\n",
    "ax[1].set_ylabel(\"Number of Best Shows\")\n",
    "ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yearly Trend of Best Movies and Shows:\n",
    "Best Movies:\n",
    "\n",
    "We can observe that the years from the late 1990s to the mid-2010s have a high number of movies that made it to the \"best\" list. There seems to be a peak around 2004.\n",
    "The last couple of years seem to have fewer movies making the cut, possibly due to evolving selection criteria or increased competition from other platforms.\n",
    "Best Shows:\n",
    "\n",
    "The distribution of best shows is quite different from movies. The late 2010s, particularly 2017 and 2018, have the highest number of shows on the \"best\" list.\n",
    "This might indicate that Netflix has been investing more in high-quality show content in recent years, or the criteria for selecting best shows might favor recent releases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IMDb score distribution for best movies, best shows, and the overall catalog\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# IMDb scores for Best Movies\n",
    "sns.histplot(best_movies_merged['imdb_score'], kde=True, bins=30, ax=ax[0], color='blue')\n",
    "ax[0].set_title(\"IMDb Score Distribution of Best Movies on Netflix\")\n",
    "ax[0].set_xlabel(\"IMDb Score\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# IMDb scores for Best Shows\n",
    "sns.histplot(best_shows_merged['imdb_score'], kde=True, bins=30, ax=ax[1], color='green')\n",
    "ax[1].set_title(\"IMDb Score Distribution of Best Shows on Netflix\")\n",
    "ax[1].set_xlabel(\"IMDb Score\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "# IMDb scores for Overall Catalog\n",
    "sns.histplot(titles_df['imdb_score'], kde=True, bins=30, ax=ax[2], color='gray')\n",
    "ax[2].set_title(\"IMDb Score Distribution of Overall Netflix Catalog\")\n",
    "ax[2].set_xlabel(\"IMDb Score\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDb Score Distribution Insights:\n",
    "Best Movies:\n",
    "\n",
    "Most of the \"best\" movies on Netflix have IMDb scores ranging from 7 to 9.\n",
    "There's a noticeable peak around the score of 8.\n",
    "Best Shows:\n",
    "\n",
    "The IMDb scores for the best shows are more evenly distributed between 7.5 and 9.\n",
    "There are peaks around scores of 8 and 8.5, indicating a significant number of shows that have received these ratings.\n",
    "Overall Catalog:\n",
    "\n",
    "The overall catalog has a broader distribution of IMDb scores, with many titles having scores between 5 and 8.\n",
    "This distribution is expected, given that it includes all titles and not just the \"best\" ones. It's evident that the \"best\" titles have higher average scores compared to the overall catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot runtime distribution for best movies and best shows\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Runtime for Best Movies\n",
    "sns.histplot(best_movies_merged['runtime'], kde=True, bins=50, ax=ax[0], color='blue')\n",
    "ax[0].set_title(\"Runtime Distribution of Best Movies on Netflix\")\n",
    "ax[0].set_xlabel(\"Runtime (in minutes)\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Runtime for Best Shows (Only considering non-zero runtimes)\n",
    "sns.histplot(best_shows_merged[best_shows_merged['runtime'] > 0]['runtime'], kde=True, bins=50, ax=ax[1], color='green')\n",
    "ax[1].set_title(\"Runtime Distribution of Best Shows on Netflix\")\n",
    "ax[1].set_xlabel(\"Runtime (in minutes)\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime Distribution Insights:\n",
    "Best Movies:\n",
    "\n",
    "The majority of the \"best\" movies on Netflix have a runtime between 80 to 150 minutes.\n",
    "There are noticeable peaks around 90 minutes and 120 minutes, indicating that movies with these runtimes are more common in the \"best\" category.\n",
    "Best Shows:\n",
    "\n",
    "The distribution for the best shows is quite different from movies. Most of the best shows have episodes with a runtime between 20 to 60 minutes.\n",
    "There's a significant peak around 45-50 minutes, suggesting that many of the best shows have episodes that are approximately this long.\n",
    "From this, we can infer that for movies, viewers tend to prefer films that are about 1.5 to 2.5 hours long, while for shows, episode lengths of around 45 minutes seem to be popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Age Certification distribution for best movies and best shows\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Age Certification for Best Movies\n",
    "sns.countplot(data=best_movies_merged, x=\"age_certification\", ax=ax[0], color='blue', order=best_movies_merged['age_certification'].value_counts().index)\n",
    "ax[0].set_title(\"Age Certification Distribution of Best Movies on Netflix\")\n",
    "ax[0].set_xlabel(\"Age Certification\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Age Certification for Best Shows\n",
    "sns.countplot(data=best_shows_merged, x=\"age_certification\", ax=ax[1], color='green', order=best_shows_merged['age_certification'].value_counts().index)\n",
    "ax[1].set_title(\"Age Certification Distribution of Best Shows on Netflix\")\n",
    "ax[1].set_xlabel(\"Age Certification\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Certification Distribution Insights:\n",
    "Best Movies:\n",
    "\n",
    "The \"best\" movies on Netflix span a range of age certifications.\n",
    "The most prevalent age certification for the best movies is R, indicating that a lot of mature content makes it to the \"best\" list.\n",
    "This is followed by PG-13, PG, and NR (Not Rated).\n",
    "Best Shows:\n",
    "\n",
    "Similar to movies, the age certification R dominates the \"best\" shows category, suggesting a preference for mature content.\n",
    "TV-MA (Mature Audiences) is also a prevalent age certification for the best shows.\n",
    "TV-14 and PG-13 are also common, suggesting that there is a mix of content suitable for different age groups.\n",
    "From this, we can infer that mature content (rated R or TV-MA) seems to be highly regarded among Netflix viewers, as a significant portion of it makes it to the \"best\" lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Release Year distribution for best movies and best shows\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Release Year for Best Movies\n",
    "sns.histplot(best_movies_merged['release_year'], kde=True, bins=50, ax=ax[0], color='blue')\n",
    "ax[0].set_title(\"Release Year Distribution of Best Movies on Netflix\")\n",
    "ax[0].set_xlabel(\"Release Year\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Release Year for Best Shows\n",
    "sns.histplot(best_shows_merged['release_year'], kde=True, bins=50, ax=ax[1], color='green')\n",
    "ax[1].set_title(\"Release Year Distribution of Best Shows on Netflix\")\n",
    "ax[1].set_xlabel(\"Release Year\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release Year Distribution Insights:\n",
    "Best Movies:\n",
    "\n",
    "The best movies on Netflix come from a wide range of release years, from classics to contemporary films.\n",
    "There's a noticeable increase in movies from the mid-1990s onwards, with peaks around the early 2000s and the 2010s.\n",
    "This suggests that while there are classics that remain popular, a lot of newer movies from the last two decades have been well-received by viewers.\n",
    "Best Shows:\n",
    "\n",
    "The distribution for best shows has a different pattern. We can observe a significant increase in the number of shows from the late 2000s and especially the 2010s.\n",
    "This indicates that many of the best shows on Netflix are relatively newer, reflecting the recent surge in high-quality TV series production and possibly Netflix's own investments in original content.\n",
    "From the insights gained, we can conclude that while Netflix has a mix of classic and contemporary movies that viewers appreciate, its TV show content seems to be more recent and is especially strong in the 2010s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the most popular genres of \"best\" movies in each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the decade\n",
    "raw_titles_df['decade'] = (10 * (raw_titles_df['release_year'] // 10)).astype(str) + 's'\n",
    "\n",
    "# Merge the 'best movies' data with the raw titles to get genre information for each best movie\n",
    "best_movies_with_genres = pd.merge(best_movies_df, raw_titles_df, left_on='title', right_on='title')\n",
    "\n",
    "# Extract the genres for each movie\n",
    "best_movies_with_genres['genres'] = best_movies_with_genres['genres'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Explode the dataframe on the genres to have one genre per row\n",
    "best_movies_exploded = best_movies_with_genres.explode('genres')\n",
    "\n",
    "# Group by decade and genre and count the number of movies\n",
    "genre_decade_counts = best_movies_exploded.groupby(['decade', 'genres']).size().reset_index(name='counts')\n",
    "\n",
    "# Pivot the dataframe to have decades as columns and genres as rows\n",
    "genre_decade_pivot = genre_decade_counts.pivot(index='genres', columns='decade', values='counts').fillna(0)\n",
    "\n",
    "# Sort the genres by their total count across all decades\n",
    "genre_decade_pivot = genre_decade_pivot.loc[genre_decade_pivot.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "genre_decade_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw_titles.csv file for the analysis\n",
    "raw_titles_df = pd.read_csv(os.path.join(directory, 'raw_titles.csv'))\n",
    "\n",
    "# Load best movies dataset\n",
    "best_movies_df = pd.read_csv(os.path.join(directory, 'Best Movies Netflix.csv'))\n",
    "\n",
    "# Create a column for the decade\n",
    "raw_titles_df['decade'] = (10 * (raw_titles_df['release_year'] // 10)).astype(str) + 's'\n",
    "\n",
    "# Merge the 'best movies' data with the raw titles to get genre information for each best movie\n",
    "best_movies_with_genres = pd.merge(best_movies_df, raw_titles_df, left_on='title', right_on='title')\n",
    "\n",
    "# Extract the genres for each movie\n",
    "best_movies_with_genres['genres'] = best_movies_with_genres['genres'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Explode the dataframe on the genres to have one genre per row\n",
    "best_movies_exploded = best_movies_with_genres.explode('genres')\n",
    "\n",
    "# Group by decade and genre and count the number of movies\n",
    "genre_decade_counts = best_movies_exploded.groupby(['decade', 'genres']).size().reset_index(name='counts')\n",
    "\n",
    "# Pivot the dataframe to have decades as columns and genres as rows\n",
    "genre_decade_pivot = genre_decade_counts.pivot(index='genres', columns='decade', values='counts').fillna(0)\n",
    "\n",
    "# Sort the genres by their total count across all decades\n",
    "genre_decade_pivot = genre_decade_pivot.loc[genre_decade_pivot.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "genre_decade_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1950s:\n",
    "Dominated by comedy, drama, and music.\n",
    "1960s:\n",
    "This decade saw a mix, with drama, action, crime, romance, and western genres being prominent.\n",
    "1970s:\n",
    "Comedy, crime, action, fantasy, horror, animation, and drama were notable genres.\n",
    "1980s:\n",
    "Drama continues its dominance, accompanied by comedy, crime, documentation, horror, war, and western.\n",
    "1990s:\n",
    "Drama still leads, followed by comedy, romance, thriller, action, european, and scifi.\n",
    "2000s:\n",
    "This decade saw a significant rise in movies, with drama, comedy, crime, romance, action, and thriller being the top genres.\n",
    "2010s:\n",
    "A massive surge in movie production. Drama leads the charts, followed by comedy, thriller, crime, action, and romance.\n",
    "2020s:\n",
    "While this decade is still young, drama, crime, comedy, documentation, thriller, and action have emerged as the leading genres.\n",
    "From the above data, it's evident that drama has been consistently popular across all decades, showcasing the genre's timeless appeal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/81hplzs570d_45ldwbxvmx4w0000gp/T/ipykernel_41370/2363128941.py:23: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from settings import (\n",
    "    DATABASE_DRIVER,\n",
    "    DATABASE_USERNAME,\n",
    "    DATABASE_PASSWORD,\n",
    "    DATABASE_HOST,\n",
    "    DATABASE_PORT,\n",
    "    DATABASE_NAME,\n",
    ")\n",
    "\n",
    "# MySQL_DATABASE_URL = f\"{settings.DATABASE_DRIVER}://{settings.DATABASE_USERNAME}:{settings.DATABASE_PASSWORD}@{settings.DATABASE_HOST}:{settings.DATABASE_PORT}/{settings.DATABASE_NAME}\"\n",
    "MySQL_DATABASE_URL = f\"{DATABASE_DRIVER}://{DATABASE_USERNAME}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}\"\n",
    "\n",
    "# engine = create_engine(MySQL_DATABASE_URL, connect_args={\"check_same_thread\": False})\n",
    "engine = create_engine(MySQL_DATABASE_URL)\n",
    "\n",
    "# engine\n",
    "\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String\n",
    "# from database import Base\n",
    "\n",
    "\n",
    "class Books(Base):\n",
    "    __tablename__ = \"books\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    title = Column(String(20))\n",
    "    author = Column(String(20))\n",
    "    description = Column(String(20))\n",
    "    rating = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel, Field\n",
    "# import models\n",
    "# from database import engine, SessionLocal\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# models.Base.metadata.create_all(bind=engine)\n",
    "\n",
    "\n",
    "def get_db():\n",
    "    try:\n",
    "        db = SessionLocal()\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str = Field(min_length=1)\n",
    "    author: str = Field(min_length=1, max_length=100)\n",
    "    description: str = Field(min_length=1, max_length=100)\n",
    "    rating: int = Field(gt=-1, lt=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
